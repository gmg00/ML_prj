{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def S(x):\n",
    "    Sigmoid = 1/(1 + np.exp(-x))\n",
    "    return Sigmoid\n",
    "\n",
    "def dS(x):\n",
    "    dSigmoid = S*(1-S)\n",
    "    return dSigmoid\n",
    "\n",
    "def forward_prop(W1,b1,W2,b2):      #W1 = W_ji , W2 = W_kj , b1 = W_j0 , b2 = W_k0\n",
    "    Net1 = W1.dot(i) + b1         # i = input  ,  y = target  ,  Net1 = Net(ji)  , Net2 = Net(kj)\n",
    "    o1 = S(Net1)\n",
    "    Net2 = W2.dot(o1) + b2\n",
    "    o2 = S(Net2)   #Qua andrebbe usata la softmax o altro per l'errore ma come placeholder ho tenuto la sigmoidale.\n",
    "    return Net1,o1,Net2,o2\n",
    "\n",
    "def backward_prop(Net1,o1,Net2,o2,W1,W2,i,y):\n",
    "    dE = y - o2\n",
    "    dW2 = (dE * dS(Net2)) * o1\n",
    "    dW1 = (dE.dot(W2) * dS(Net1)) * i\n",
    "    db2 = (dE * dS(Net2))\n",
    "    db1 = (dE.dot(W2) * dS(Net1))\n",
    "\n",
    "def update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,alpha):   #Giusto abbozzato ovviamente\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * b1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * b2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
