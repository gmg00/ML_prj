{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_act_out(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def f_act_hidden(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, input, hidden_units, output_units):\n",
    "        self.input = input\n",
    "        self.hidden = np.random.random((hidden_units, input.shape[1]))\n",
    "        self.output = np.random.random((output_units, input.shape[1]))\n",
    "        self.w_ji = np.random.random((hidden_units, input.shape[0]))\n",
    "        self.bias_ji = np.random.random((hidden_units, 1))\n",
    "        self.w_kj = np.random.random((output_units, hidden_units))\n",
    "        self.bias_kj = np.random.random((output_units, 1))\n",
    "        #Da mettere i bias\n",
    "        #Possiamo anche mettere un 1 in capo al vettore input e il bias contarlo nella matrice w.\n",
    "\n",
    "    def forward_prop(self):\n",
    "\n",
    "        self.hidden = f_act_hidden(self.w_ji @ self.input + self.bias_ji)  # hidden x N\n",
    "        self.output = f_act_out(self.w_kj @ self.hidden + self.bias_kj) #+ bias #output x N\n",
    "    \n",
    "    def backward_prop(self):\n",
    "        \n",
    "        delta_k = (self.output - self.target) * np.gradient(self.output)[1] # output x N\n",
    "        delta_j = np.sum(delta_k @ self.w_kj)\n",
    "        Dw_kj = delta_k @ self.hidden.T # output x N @ N x hidden = output x hidden \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "N = 100 #dimensione pattern\n",
    "dim_input = 5 #dimensione input\n",
    "dim_hl = 3 #dimensione hidden layer\n",
    "dim_ol = 2 #dimensione output layer\n",
    "\n",
    "input = np.ones((dim_input,N))\n",
    "target = np.ones((dim_ol,N))\n",
    "\n",
    "w_ji = np.ones((dim_hl, dim_input))\n",
    "w_kj = np.ones((dim_ol, dim_hl))\n",
    "\n",
    "hidden = np.ones((dim_hl,N))\n",
    "output = np.ones((dim_ol,N))\n",
    "\n",
    "def f_act_out(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def f_act_hidden(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#Forward computation\n",
    "hidden = f_act_hidden(w_ji @ input)\n",
    "output = f_act_out(w_kj @ hidden)\n",
    "print(output.shape)\n",
    "#print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  2.,  6., 12., 20., 30.],\n",
       "        [ 0.,  2.,  6., 12., 20., 30.]]),\n",
       " array([[ 1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 3.,  4.,  6.,  8., 10., 11.]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5,6],[1,4,9,16,25,36]])\n",
    "print(a.shape)\n",
    "np.gradient(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.gradient(output))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n",
      "(3, 5)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "#Computation of errors at output layer\n",
    "#print(np.array(np.gradient(output)))\n",
    "delta_k = (-output+target) * (output)\n",
    "print(delta_k.shape)\n",
    "Dw_kj = delta_k @ hidden.T\n",
    "delta_j = (w_kj.T @ delta_k) * hidden #delta_j deve essere N x input \n",
    "Dw_ji = delta_j @ input.T\n",
    "print(Dw_ji.shape)\n",
    "print(w_ji.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_kj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(units, act_function):\n",
    "        self.units = units\n",
    "        self.act_function = act_function\n",
    "\n",
    "        self.layer = np.empty((units, 1))\n",
    "    \n",
    "    def update_w(Dw)\n",
    "\n",
    "class NeuralNetwork(Layer):\n",
    "    def __init__(self, input, hidden_units, output_units):\n",
    "        self.input = input\n",
    "        self.hidden = np.random.random((hidden_units, input.shape[1]))\n",
    "        #self.hidden = Layer(hidden_units, act_function)\n",
    "        self.output = np.random.random((output_units, input.shape[1]))\n",
    "        #self.output = Layer(output_units, act_function)\n",
    "\n",
    "        self.w_ji = np.random.random((hidden_units, input.shape[0]))\n",
    "        self.bias_ji = np.random.random((hidden_units, 1))\n",
    "\n",
    "        self.w_kj = np.random.random((output_units, hidden_units))\n",
    "        self.bias_kj = np.random.random((output_units, 1))\n",
    "        \n",
    "        #Da mettere i bias\n",
    "        #Possiamo anche mettere un 1 in capo al vettore input e il bias contarlo nella matrice w.\n",
    "\n",
    "    def forward_prop(self):\n",
    "\n",
    "        self.hidden = f_act_hidden(self.w_ji @ self.input + self.bias_ji)  # hidden x N\n",
    "        self.output = f_act_out(self.w_kj @ self.hidden + self.bias_kj) #+ bias #output x N\n",
    "    \n",
    "    def backward_prop(self):\n",
    "        \n",
    "        delta_k = (self.output - self.target) * np.gradient(self.output)[1] # output x N\n",
    "        delta_j = np.sum(delta_k @ self.w_kj)\n",
    "        Dw_kj = delta_k @ self.hidden.T # output x N @ N x hidden = output x hidden \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "layers_a = []\n",
    "layers_z = []\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "d_weights = []\n",
    "d_biases = []\n",
    "\n",
    "N = 100 #batch dimension\n",
    "input_dim = 5 \n",
    "hidden_dim = 8\n",
    "output_dim = 3\n",
    "layer_dim = [hidden_dim, output_dim]\n",
    "\n",
    "for i in range(len(layer_dim)):\n",
    "    layers_a.append(np.empty((layer_dim[i],N)))\n",
    "    layers_z.append(np.empty((layer_dim[i],N)))\n",
    "\n",
    "input = np.random.random((input_dim,N))\n",
    "y = np.random.randint(0,2,(output_dim,N))\n",
    "\n",
    "w_ji = np.random.uniform(-0.3,0.3,(hidden_dim,input_dim))\n",
    "weights.append(w_ji)\n",
    "d_weights.append(np.empty((hidden_dim,input_dim)))\n",
    "w_kj = np.random.uniform(-0.3,0.3,(output_dim,hidden_dim))\n",
    "weights.append(w_kj)\n",
    "d_weights.append(np.empty((output_dim,hidden_dim)))\n",
    "\n",
    "b_j = np.ones((hidden_dim,1))\n",
    "biases.append(b_j)\n",
    "d_biases.append(np.empty((hidden_dim,1)))\n",
    "b_k = np.ones((output_dim,1))\n",
    "biases.append(b_k)\n",
    "d_biases.append(np.empty((output_dim,1)))\n",
    "\n",
    "act_functions = [sigmoid, sigmoid]\n",
    "d_act_functions = [d_sigmoid, d_sigmoid]\n",
    "\n",
    "#forward propagation\n",
    "for l in range(len(layer_dim)):\n",
    "    if l == 0:\n",
    "        layers_z[l] = weights[l] @ input + biases[l]\n",
    "        layers_a[l] = act_functions[l](layers_z[l])\n",
    "    else:\n",
    "        layers_z[l] = weights[l] @ layers_a[l-1] + biases[l]\n",
    "        layers_a[l] = act_functions[l](layers_z[l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "(8, 100)\n",
      "Layer 1\n",
      "(3, 100)\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(layers_a):\n",
    "    print(f'Layer {i}')\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.67689876  1.76818167  1.73860926  2.05389029  1.59689574]\n",
      " [-0.45594775 -0.75782144 -0.38195253 -0.84082482 -0.26670657]\n",
      " [ 0.74369032  0.62069796  0.85378303  0.65032114  0.87043844]\n",
      " [-1.53860947 -1.88154319 -1.54227131 -2.04571599 -1.378     ]\n",
      " [ 2.13506102  2.19674055  2.28474158  2.38795856  2.18191644]\n",
      " [-0.43599989 -0.4573967  -0.41259936 -0.66117457 -0.32919073]\n",
      " [ 1.42056247  1.56919431  1.44434125  1.82914517  1.29851659]\n",
      " [-0.75394905 -0.95175188 -0.71680472 -1.12680001 -0.59379996]]\n",
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "deltas = []\n",
    "deltas.append(np.empty((hidden_dim,N)))\n",
    "deltas.append(np.empty((output_dim,N)))\n",
    "#backpropagation\n",
    "for l in range(len(layer_dim)-1,-1,-1):\n",
    "    if l == len(layer_dim)-1:\n",
    "        deltas[l] = 2*d_act_functions[l](layers_z[l])*(layers_a[l]-y)\n",
    "    else:\n",
    "        deltas[l] = weights[l+1].T @ deltas[l+1]\n",
    "\n",
    "for i in range(len(d_weights)):\n",
    "    if i == 0:\n",
    "        d_weights[i] = deltas[i] @ input.T\n",
    "    else: \n",
    "        d_weights[i] = deltas[i] @ layers_a[i-1].T\n",
    "\n",
    "print(d_weights[0])\n",
    "print(d_weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99627385,  0.99768947,  0.99777273, -0.00249981,  0.99732848,\n",
       "         0.99674436, -0.00218962,  0.99702821,  0.99768665,  0.99759335],\n",
       "       [-0.00438532,  0.99732753, -0.00262682, -0.00297894, -0.00307943,\n",
       "         0.99605217,  0.99745861, -0.00350356, -0.00273181,  0.99716878],\n",
       "       [ 0.97736136,  0.98308885,  0.98333755, -0.0177775 ,  0.98162346,\n",
       "        -0.02092771,  0.98366243,  0.98004573, -0.01702208, -0.01744107]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_a[1]-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight update\n",
    "for i,w in enumerate(weights):\n",
    "    d_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "k = 0\n",
    "a = weights[l+1].T @ (act_functions[l+1](layers_z[l+1]) * (2*(layers_a[l+1]-y)))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights[l+1][k].T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(act_functions[l+1](layers_z[l+1][k])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*(layers_a[l+1][k]-y[k])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4],\n",
       "       [ 2,  4,  6,  8],\n",
       "       [ 3,  6,  9, 12]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3]).reshape((3,1))\n",
    "b = np.array([1,2,3,4]).reshape((1,4))\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "(8, 1)\n",
      "Layer 1\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(layers_a):\n",
    "    print(f'Layer {i}')\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m         tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(layer_dim[l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m      9\u001b[0m                     \u001b[38;5;66;03m#1xj                                #1xN\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m             tmp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers_z\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlayers_a\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         delta_j \u001b[38;5;241m=\u001b[39m act_functions[l](layers_z[l])\u001b[38;5;241m*\u001b[39mtmp\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(delta_k\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 8)"
     ]
    }
   ],
   "source": [
    "\n",
    "#backpropagation\n",
    "for l in range(len(layer_dim)-1,-1,-1):\n",
    "    if l == len(layer_dim)-1:\n",
    "                                #kxN                    #kxN\n",
    "        delta_k = 2*act_functions[l](layers_z[l])*(layers_a[l]-y)\n",
    "    else:\n",
    "        tmp = 0\n",
    "        for k in range(layer_dim[l+1]):\n",
    "                    #1xj                                #1xN\n",
    "            tmp += weights[l+1][k,:].T @ (act_functions[l+1](layers_z[l+1][k,:]) * (2*(layers_a[l+1][k,:]-y[k,:])))\n",
    "        delta_j = act_functions[l](layers_z[l])*tmp\n",
    "\n",
    "print(delta_k.shape)\n",
    "print(delta_j.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
