{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_act_out(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def f_act_hidden(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, input, hidden_units, output_units):\n",
    "        self.input = input\n",
    "        self.hidden = np.random.random((hidden_units, input.shape[1]))\n",
    "        self.output = np.random.random((output_units, input.shape[1]))\n",
    "        self.w_ji = np.random.random((hidden_units, input.shape[0]))\n",
    "        self.bias_ji = np.random.random((hidden_units, 1))\n",
    "        self.w_kj = np.random.random((output_units, hidden_units))\n",
    "        self.bias_kj = np.random.random((output_units, 1))\n",
    "        #Da mettere i bias\n",
    "        #Possiamo anche mettere un 1 in capo al vettore input e il bias contarlo nella matrice w.\n",
    "\n",
    "    def forward_prop(self):\n",
    "\n",
    "        self.hidden = f_act_hidden(self.w_ji @ self.input + self.bias_ji)  # hidden x N\n",
    "        self.output = f_act_out(self.w_kj @ self.hidden + self.bias_kj) #+ bias #output x N\n",
    "    \n",
    "    def backward_prop(self):\n",
    "        \n",
    "        delta_k = (self.output - self.target) * np.gradient(self.output)[1] # output x N\n",
    "        delta_j = np.sum(delta_k @ self.w_kj)\n",
    "        Dw_kj = delta_k @ self.hidden.T # output x N @ N x hidden = output x hidden \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n"
     ]
    }
   ],
   "source": [
    "N = 100 #dimensione pattern\n",
    "dim_input = 5 #dimensione input\n",
    "dim_hl = 3 #dimensione hidden layer\n",
    "dim_ol = 2 #dimensione output layer\n",
    "\n",
    "input = np.ones((dim_input,N))\n",
    "target = np.ones((dim_ol,N))\n",
    "\n",
    "w_ji = np.ones((dim_hl, dim_input))\n",
    "w_kj = np.ones((dim_ol, dim_hl))\n",
    "\n",
    "hidden = np.ones((dim_hl,N))\n",
    "output = np.ones((dim_ol,N))\n",
    "\n",
    "def f_act_out(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def f_act_hidden(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#Forward computation\n",
    "hidden = f_act_hidden(w_ji @ input)\n",
    "output = f_act_out(w_kj @ hidden)\n",
    "print(output.shape)\n",
    "#print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  2.,  6., 12., 20., 30.],\n",
       "        [ 0.,  2.,  6., 12., 20., 30.]]),\n",
       " array([[ 1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 3.,  4.,  6.,  8., 10., 11.]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5,6],[1,4,9,16,25,36]])\n",
    "print(a.shape)\n",
    "np.gradient(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.gradient(output))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100)\n",
      "(3, 5)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "#Computation of errors at output layer\n",
    "#print(np.array(np.gradient(output)))\n",
    "delta_k = (-output+target) * (output)\n",
    "print(delta_k.shape)\n",
    "Dw_kj = delta_k @ hidden.T\n",
    "delta_j = (w_kj.T @ delta_k) * hidden #delta_j deve essere N x input \n",
    "Dw_ji = delta_j @ input.T\n",
    "print(Dw_ji.shape)\n",
    "print(w_ji.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_kj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(units, act_function):\n",
    "        self.units = units\n",
    "        self.act_function = act_function\n",
    "\n",
    "        self.layer = np.empty((units, 1))\n",
    "    \n",
    "    def update_w(Dw)\n",
    "\n",
    "class NeuralNetwork(Layer):\n",
    "    def __init__(self, input, hidden_units, output_units):\n",
    "        self.input = input\n",
    "        self.hidden = np.random.random((hidden_units, input.shape[1]))\n",
    "        #self.hidden = Layer(hidden_units, act_function)\n",
    "        self.output = np.random.random((output_units, input.shape[1]))\n",
    "        #self.output = Layer(output_units, act_function)\n",
    "\n",
    "        self.w_ji = np.random.random((hidden_units, input.shape[0]))\n",
    "        self.bias_ji = np.random.random((hidden_units, 1))\n",
    "\n",
    "        self.w_kj = np.random.random((output_units, hidden_units))\n",
    "        self.bias_kj = np.random.random((output_units, 1))\n",
    "        \n",
    "        #Da mettere i bias\n",
    "        #Possiamo anche mettere un 1 in capo al vettore input e il bias contarlo nella matrice w.\n",
    "\n",
    "    def forward_prop(self):\n",
    "\n",
    "        self.hidden = f_act_hidden(self.w_ji @ self.input + self.bias_ji)  # hidden x N\n",
    "        self.output = f_act_out(self.w_kj @ self.hidden + self.bias_kj) #+ bias #output x N\n",
    "    \n",
    "    def backward_prop(self):\n",
    "        \n",
    "        delta_k = (self.output - self.target) * np.gradient(self.output)[1] # output x N\n",
    "        delta_j = np.sum(delta_k @ self.w_kj)\n",
    "        Dw_kj = delta_k @ self.hidden.T # output x N @ N x hidden = output x hidden \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "layers_a = []\n",
    "layers_z = []\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "d_weights = []\n",
    "d_biases = []\n",
    "\n",
    "N = 100 #batch dimension\n",
    "input_dim = 5 \n",
    "hidden_dim = 8\n",
    "output_dim = 3\n",
    "layer_dim = [hidden_dim, output_dim]\n",
    "\n",
    "for i in range(len(layer_dim)):\n",
    "    layers_a.append(np.empty((layer_dim[i],N)))\n",
    "    layers_z.append(np.empty((layer_dim[i],N)))\n",
    "\n",
    "input = np.random.random((input_dim,N))\n",
    "y = np.random.randint(0,2,(output_dim,N))\n",
    "\n",
    "w_ji = np.random.uniform(-0.3,0.3,(hidden_dim,input_dim))\n",
    "weights.append(w_ji)\n",
    "d_weights.append(np.empty((hidden_dim,input_dim)))\n",
    "w_kj = np.random.uniform(-0.3,0.3,(output_dim,hidden_dim))\n",
    "weights.append(w_kj)\n",
    "d_weights.append(np.empty((output_dim,hidden_dim)))\n",
    "\n",
    "b_j = np.ones((hidden_dim,1))\n",
    "biases.append(b_j)\n",
    "d_biases.append(np.empty((hidden_dim,1)))\n",
    "b_k = np.ones((output_dim,1))\n",
    "biases.append(b_k)\n",
    "d_biases.append(np.empty((output_dim,1)))\n",
    "\n",
    "act_functions = [sigmoid, sigmoid]\n",
    "d_act_functions = [d_sigmoid, d_sigmoid]\n",
    "\n",
    "#forward propagation\n",
    "for l in range(len(layer_dim)):\n",
    "    if l == 0:\n",
    "        layers_z[l] = weights[l] @ input + biases[l]\n",
    "        layers_a[l] = act_functions[l](layers_z[l])\n",
    "    else:\n",
    "        layers_z[l] = weights[l] @ layers_a[l-1] + biases[l]\n",
    "        layers_a[l] = act_functions[l](layers_z[l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "(8, 100)\n",
      "Layer 1\n",
      "(3, 100)\n"
     ]
    }
   ],
   "source": [
    "for i,a in enumerate(layers_a):\n",
    "    print(f'Layer {i}')\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00662616  0.00637228  0.00738034  0.00656117  0.00549375]\n",
      " [-0.00013578 -0.00017834 -0.00179814 -0.00084971 -0.00219556]\n",
      " [ 0.01820403  0.01612615  0.01666229  0.01376057  0.01351528]\n",
      " [-0.01185664 -0.01150553 -0.01260115 -0.01173269 -0.0087819 ]\n",
      " [-0.00868432 -0.00777199 -0.00681573 -0.00630867 -0.0047521 ]\n",
      " [ 0.02431211  0.02184938  0.02301277  0.01932113  0.01832983]\n",
      " [-0.0070048  -0.00741169 -0.0090143  -0.00881523 -0.00583896]\n",
      " [ 0.02128513  0.01959225  0.02293638  0.01893471  0.01867783]]\n",
      "(3, 8)\n",
      "(8, 1)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "deltas = []\n",
    "deltas.append(np.empty((hidden_dim,N)))\n",
    "deltas.append(np.empty((output_dim,N)))\n",
    "#backpropagation\n",
    "for l in range(len(layer_dim)-1,-1,-1):\n",
    "    if l == len(layer_dim)-1:\n",
    "        deltas[l] = 2*d_act_functions[l](layers_z[l])*(layers_a[l]-y)/y.shape[1]\n",
    "    else:\n",
    "        deltas[l] = weights[l+1].T @ deltas[l+1]\n",
    "\n",
    "for i in range(len(d_weights)):\n",
    "    if i == 0:\n",
    "        d_weights[i] = deltas[i] @ input.T\n",
    "        d_biases[i] = deltas[i].sum(axis=1).reshape((deltas[i].shape[0],1))\n",
    "    else: \n",
    "        d_weights[i] = deltas[i] @ layers_a[i-1].T\n",
    "        d_biases[i] = deltas[i].sum(axis=1).reshape((deltas[i].shape[0],1))\n",
    "\n",
    "print(d_weights[0])\n",
    "print(d_weights[1].shape)\n",
    "print(d_biases[0].shape)\n",
    "print(d_biases[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78905826,  0.78730059,  0.78733309,  0.78616837, -0.20993073,\n",
       "        -0.21140939,  0.7859117 , -0.21303588, -0.21308077, -0.2135605 ,\n",
       "        -0.2110439 ,  0.78884225, -0.21158154,  0.78646433,  0.78993787,\n",
       "        -0.20897614, -0.21609959,  0.78995301,  0.78688812, -0.21136026,\n",
       "        -0.21391391, -0.21243251,  0.78749986,  0.78812805, -0.2118314 ,\n",
       "         0.78706355, -0.21128195, -0.2084679 , -0.21369724,  0.78788733,\n",
       "        -0.21316653,  0.7896671 , -0.21333439, -0.21049669,  0.79088574,\n",
       "        -0.21172429, -0.21243227, -0.20843329, -0.21146457,  0.78660247,\n",
       "         0.78800862, -0.21007861, -0.2141437 , -0.21124813,  0.78869404,\n",
       "         0.78656868, -0.21280227,  0.78493183,  0.7842638 , -0.21332867,\n",
       "         0.78793858, -0.21308368,  0.78754794,  0.78531495, -0.21404935,\n",
       "        -0.21315808,  0.78590645, -0.21325774,  0.7872054 , -0.21234994,\n",
       "        -0.2098902 ,  0.78481456,  0.78308358, -0.21308017, -0.2116251 ,\n",
       "        -0.21313975, -0.21323762, -0.21489639, -0.21121228,  0.78906476,\n",
       "        -0.21038602,  0.78618669,  0.78971029,  0.78668446, -0.2114423 ,\n",
       "         0.78805937,  0.78747946,  0.78688095,  0.78885105,  0.78883454,\n",
       "         0.78456594, -0.21294029,  0.78868811,  0.78857334, -0.21347185,\n",
       "        -0.21017622,  0.78812599, -0.20968129, -0.21487153, -0.21231515,\n",
       "         0.78961023,  0.78402596,  0.78908401,  0.78728258,  0.78638175,\n",
       "        -0.2134458 , -0.2104599 ,  0.78969554,  0.78838289, -0.21421006],\n",
       "       [ 0.60557518, -0.39285384,  0.61042337,  0.60753773,  0.6051094 ,\n",
       "         0.60901374,  0.61013103,  0.61063135, -0.39143953,  0.61070374,\n",
       "        -0.38939004, -0.39137004,  0.60369306, -0.3893587 ,  0.6069713 ,\n",
       "        -0.39606265, -0.38733692, -0.39424084, -0.39210881,  0.60809417,\n",
       "         0.60843963,  0.60952444,  0.60680032,  0.60736018, -0.39253127,\n",
       "        -0.39170569,  0.60642174, -0.39419231, -0.38753249,  0.60478117,\n",
       "         0.60705751,  0.60171166, -0.38972838,  0.60901318, -0.39340224,\n",
       "        -0.3936301 , -0.39224831,  0.60571124, -0.39047933,  0.60726859,\n",
       "         0.60651058, -0.3933732 ,  0.60891168, -0.39406293, -0.39517936,\n",
       "        -0.39131261,  0.61006056,  0.61118058,  0.61329516, -0.38960728,\n",
       "        -0.3931275 , -0.39220843,  0.60830547, -0.38878893, -0.38530775,\n",
       "         0.60698712, -0.38840715,  0.61119084, -0.39003317,  0.61022648,\n",
       "         0.60337359, -0.38921109, -0.38770608,  0.60570239, -0.39537082,\n",
       "         0.6083625 , -0.3934509 , -0.39222391, -0.39581791, -0.39483389,\n",
       "         0.60727814,  0.61160544,  0.60575081, -0.3921206 ,  0.60959451,\n",
       "         0.60777873, -0.39177958,  0.60874607, -0.39202695, -0.39467149,\n",
       "         0.61186558,  0.60899427,  0.60606252,  0.6064195 , -0.39256622,\n",
       "         0.60685506,  0.60810049,  0.60558423,  0.6109711 ,  0.60975464,\n",
       "         0.60634071, -0.3899985 , -0.39610426, -0.39282375, -0.39123502,\n",
       "         0.60966754,  0.605044  , -0.39631406, -0.39085641, -0.39153637],\n",
       "       [ 0.63863736,  0.63429903, -0.36013424,  0.64051729,  0.6357753 ,\n",
       "         0.64289742, -0.35630665,  0.64243713,  0.64120322,  0.63897052,\n",
       "         0.63965119, -0.36373868, -0.36518764, -0.35858151, -0.36311607,\n",
       "         0.63353025, -0.35376957, -0.36646262,  0.64041789,  0.63977374,\n",
       "        -0.36089732,  0.63841999, -0.36543505, -0.36621464,  0.64052171,\n",
       "         0.63570959,  0.63503522,  0.63557301, -0.35923621, -0.36177132,\n",
       "         0.63408822, -0.3658494 ,  0.63750875,  0.63964531,  0.63766948,\n",
       "         0.639677  ,  0.64147447,  0.63659801,  0.63987435,  0.63619544,\n",
       "         0.63776224,  0.6367366 , -0.35954158, -0.36839319,  0.63841529,\n",
       "        -0.36024228,  0.6403936 , -0.35773366,  0.64520262,  0.64261847,\n",
       "         0.63625552, -0.35876601, -0.36293692,  0.64293862, -0.3557954 ,\n",
       "         0.63904694,  0.64778148,  0.6437703 ,  0.6443665 ,  0.63862036,\n",
       "        -0.3699236 , -0.3557067 ,  0.6454664 , -0.36431726,  0.63131663,\n",
       "         0.63736307, -0.36445073, -0.36244832, -0.36332799, -0.36723963,\n",
       "         0.63516003,  0.64019022, -0.36258027,  0.63970345,  0.63861895,\n",
       "         0.64031744, -0.35781734, -0.36358208,  0.64139066, -0.36488318,\n",
       "         0.64435451, -0.35677434,  0.63260924,  0.63537862, -0.36099895,\n",
       "         0.63750526,  0.63389591,  0.63873586,  0.64401325,  0.64373343,\n",
       "         0.63934131,  0.64196173,  0.63771159,  0.64093605,  0.63938457,\n",
       "         0.639687  , -0.36638708, -0.36584349, -0.35738445, -0.35847745]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_a[1]-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 5)\n",
      "(8, 1)\n",
      "(3, 8)\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "#weight update\n",
    "eta = 0.1\n",
    "for i in range(len(weights)):\n",
    "    weights[i] = weights[i] - d_weights[i]*eta\n",
    "    biases[i] = biases[i] - d_biases[i]*eta\n",
    "    print(weights[i].shape)\n",
    "    print(biases[i].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "layers_a = []\n",
    "layers_z = []\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "d_weights = []\n",
    "d_biases = []\n",
    "\n",
    "N = 100 #batch dimension\n",
    "input_dim = 5 \n",
    "hidden_dim = 8\n",
    "output_dim = 3\n",
    "layer_dim = [hidden_dim, output_dim]\n",
    "\n",
    "for i in range(len(layer_dim)):\n",
    "    layers_a.append(np.empty((layer_dim[i],N)))\n",
    "    layers_z.append(np.empty((layer_dim[i],N)))\n",
    "\n",
    "input = np.random.random((input_dim,N))\n",
    "y = np.random.randint(0,2,(output_dim,N))\n",
    "\n",
    "w_ji = np.random.uniform(-0.3,0.3,(hidden_dim,input_dim))\n",
    "weights.append(w_ji)\n",
    "d_weights.append(np.empty((hidden_dim,input_dim)))\n",
    "w_kj = np.random.uniform(-0.3,0.3,(output_dim,hidden_dim))\n",
    "weights.append(w_kj)\n",
    "d_weights.append(np.empty((output_dim,hidden_dim)))\n",
    "\n",
    "b_j = np.ones((hidden_dim,1))\n",
    "biases.append(b_j)\n",
    "d_biases.append(np.empty((hidden_dim,1)))\n",
    "b_k = np.ones((output_dim,1))\n",
    "biases.append(b_k)\n",
    "d_biases.append(np.empty((output_dim,1)))\n",
    "\n",
    "act_functions = [sigmoid, sigmoid]\n",
    "d_act_functions = [d_sigmoid, d_sigmoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "o = 0\n",
    "while o<1000:\n",
    "    print(f'o = {o}')\n",
    "    #forward propagation\n",
    "    for l in range(len(layer_dim)):\n",
    "        if l == 0:\n",
    "            layers_z[l] = weights[l] @ input + biases[l]\n",
    "            layers_a[l] = act_functions[l](layers_z[l])\n",
    "        else:\n",
    "            layers_z[l] = weights[l] @ layers_a[l-1] + biases[l]\n",
    "            layers_a[l] = act_functions[l](layers_z[l])\n",
    "\n",
    "    deltas = []\n",
    "    deltas.append(np.empty((hidden_dim,N)))\n",
    "    deltas.append(np.empty((output_dim,N)))\n",
    "    #backpropagation\n",
    "    for l in range(len(layer_dim)-1,-1,-1):\n",
    "        if l == len(layer_dim)-1:\n",
    "            deltas[l] = 2*d_act_functions[l](layers_z[l])*(layers_a[l]-y)/y.shape[1]\n",
    "        else:\n",
    "            deltas[l] = weights[l+1].T @ deltas[l+1]\n",
    "\n",
    "    for i in range(len(d_weights)):\n",
    "        if i == 0:\n",
    "            d_weights[i] = deltas[i] @ input.T\n",
    "            d_biases[i] = deltas[i].sum(axis=1).reshape((deltas[i].shape[0],1))\n",
    "        else: \n",
    "            d_weights[i] = deltas[i] @ layers_a[i-1].T\n",
    "            d_biases[i] = deltas[i].sum(axis=1).reshape((deltas[i].shape[0],1))\n",
    "\n",
    "    print(d_weights[0])\n",
    "    print(d_weights[1].shape)\n",
    "    print(d_biases[0].shape)\n",
    "    print(d_biases[1].shape)\n",
    "    #weight update\n",
    "    eta = 0.1\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i] - d_weights[i]*eta\n",
    "        biases[i] = biases[i] - d_biases[i]*eta\n",
    "\n",
    "    o += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "#linear activation function \n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def d_linear(x):\n",
    "    return 1\n",
    "\n",
    "#sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    f =sigmoid(x)\n",
    "    return f * (1-f)\n",
    "\n",
    "#ReLu activation function\n",
    "def relu(x):\n",
    "    return np.max(0,x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return int(x>0)\n",
    "\n",
    "#hyperbolic tangent activation function\n",
    "def TanH(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_TanH(x):\n",
    "    return 1 - math.pow(np.tanh(x), 2) \n",
    "\n",
    "#Dictionary for the activation functions\n",
    "act_func = {\n",
    "    'lin': linear,\n",
    "    'sigm': sigmoid,\n",
    "    'relu': relu,\n",
    "    'tanh': TanH\n",
    "}\n",
    "\n",
    "#A second dictionary for their derivatives\n",
    "d_act_func = {\n",
    "    'lin': d_linear,\n",
    "    'sigm': d_sigmoid,\n",
    "    'relu': d_relu,\n",
    "    'tanh': d_TanH\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input, prev_layer, dim_layer, act_function, dim_batch, target):\n",
    "\n",
    "        self.dim_layer = dim_layer\n",
    "        self.prev_layer = prev_layer\n",
    "        self.act_function = act_func[act_function]\n",
    "        self.d_act_function = d_act_func[act_function]\n",
    "        self.dim_batch = dim_batch\n",
    "        self.target = target\n",
    "\n",
    "        if self.prev_layer != None:\n",
    "            self.W = np.random.uniform(-0.2, 0.2, (self.dim_layer, self.prev_layer.dim_layer))    #inizializzo la matrice dei pesi\n",
    "            self.b = np.random.uniform(-0.2, 0.2, (self.dim_layer, self.dim_batch))      #inizializzo il vettore dei bias\n",
    "            self.layer = np.empty((self.dim_layer, self.dim_batch))\n",
    "        else: \n",
    "            self.layer = input\n",
    "\n",
    "    def forward(self):\n",
    "        if self.prev_layer == None:\n",
    "            print('prev_layer == None: return self.layer')\n",
    "            return self.layer\n",
    "        else: \n",
    "            print('prev_layer =! None: compute forward')\n",
    "            self.z = self.W @ self.prev_layer.forward() + self.b\n",
    "            self.layer = self.act_function(self.z)\n",
    "            return self.layer\n",
    "    \n",
    "    def backward(self, next_delta, next_weights):\n",
    "        print(f'Entered backward: target = {self.target}')\n",
    "        \n",
    "        if self.target is None:\n",
    "            if self.prev_layer != None:\n",
    "                print('self.target == None: hidden')\n",
    "                delta = next_weights.T @ next_delta\n",
    "                #self.prev_layer.backward(delta,self.weights)\n",
    "                self.d_W = delta @ self.prev_layer.backward(delta,self.W).T\n",
    "                self.d_b = delta.sum(axis=1).reshape((delta.shape[0],1))\n",
    "                return self.layer\n",
    "\n",
    "            else: \n",
    "                print('input')\n",
    "                return self.layer\n",
    "            \n",
    "        else:\n",
    "            print('self.target != None: output')\n",
    "            delta = 2 * self.d_act_function(self.z) * (self.layer - self.target)/self.target.shape[1]\n",
    "            #self.prev_layer.backward(delta,self.weights)\n",
    "            self.d_W = delta @ self.prev_layer.backward(delta,self.W).T\n",
    "            self.d_b = delta.sum(axis=1).reshape((delta.shape[0],1))\n",
    "            return self.layer    \n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        if self.prev_layer is None:\n",
    "            return\n",
    "        else:  \n",
    "            self.W = self.W - eta * self.d_W\n",
    "            self.prev_layer.update_weights(eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_layer \u001b[38;5;241m=\u001b[39m \u001b[43mLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m hidden_layer \u001b[38;5;241m=\u001b[39m Layer(\u001b[38;5;28;01mNone\u001b[39;00m, input_layer, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m Layer(\u001b[38;5;28;01mNone\u001b[39;00m, hidden_layer, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m, y)\n",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, input, prev_layer, dim_layer, act_function, dim_batch, target)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_layer \u001b[38;5;241m=\u001b[39m dim_layer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_layer \u001b[38;5;241m=\u001b[39m prev_layer\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_function \u001b[38;5;241m=\u001b[39m \u001b[43mact_func\u001b[49m[act_function]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_act_function \u001b[38;5;241m=\u001b[39m d_act_func[act_function]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_batch \u001b[38;5;241m=\u001b[39m dim_batch\n",
      "\u001b[1;31mNameError\u001b[0m: name 'act_func' is not defined"
     ]
    }
   ],
   "source": [
    "input_layer = Layer(input, None, 5, 'lin', 100, None)\n",
    "hidden_layer = Layer(None, input_layer, 8, 'sigm', 100, None)\n",
    "output_layer = Layer(None, hidden_layer, 3, 'sigm', 100, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_layer =! None: compute forward\n",
      "prev_layer =! None: compute forward\n",
      "prev_layer == None: return self.layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.55308483, 0.57060642, 0.57127585, 0.51116767, 0.51429673,\n",
       "        0.58918799, 0.5320979 , 0.52437182, 0.55960736, 0.53453741,\n",
       "        0.52303894, 0.52668929, 0.52469992, 0.50985699, 0.53433008,\n",
       "        0.52463116, 0.58410699, 0.58704386, 0.53038132, 0.52614839,\n",
       "        0.60276981, 0.58672433, 0.60066383, 0.51600162, 0.54748058,\n",
       "        0.58215144, 0.5860736 , 0.52325247, 0.57896977, 0.52504101,\n",
       "        0.57153595, 0.57628479, 0.56737654, 0.53191928, 0.5990363 ,\n",
       "        0.57311031, 0.59301413, 0.58961265, 0.50747044, 0.57886493,\n",
       "        0.54382195, 0.60183189, 0.58733892, 0.54705752, 0.55410878,\n",
       "        0.54019972, 0.51146949, 0.57068685, 0.51167995, 0.58144989,\n",
       "        0.53680763, 0.55732031, 0.54038827, 0.52528295, 0.59493734,\n",
       "        0.56915354, 0.57106836, 0.50627798, 0.56302076, 0.59119437,\n",
       "        0.57284544, 0.51462158, 0.54218939, 0.51170172, 0.56226095,\n",
       "        0.60202234, 0.51787829, 0.5309385 , 0.54878633, 0.55744983,\n",
       "        0.60680293, 0.53240159, 0.59568601, 0.51172813, 0.58976035,\n",
       "        0.52568895, 0.56511393, 0.50511471, 0.58883373, 0.55510401,\n",
       "        0.55973194, 0.57970759, 0.51237465, 0.56802848, 0.5815896 ,\n",
       "        0.60130245, 0.50763096, 0.58611255, 0.59718717, 0.5488155 ,\n",
       "        0.55244577, 0.54646379, 0.51782068, 0.57884908, 0.5638776 ,\n",
       "        0.50814385, 0.58711176, 0.5309187 , 0.59795402, 0.55845153],\n",
       "       [0.52657221, 0.53500188, 0.51328728, 0.49530477, 0.5406888 ,\n",
       "        0.52003072, 0.55383805, 0.48907033, 0.48572648, 0.5732396 ,\n",
       "        0.54811468, 0.54065889, 0.48284001, 0.52617236, 0.47634725,\n",
       "        0.5057189 , 0.55015004, 0.51106524, 0.52614458, 0.51739419,\n",
       "        0.53177554, 0.50429904, 0.54428501, 0.47483879, 0.54218597,\n",
       "        0.49345546, 0.5612088 , 0.54413167, 0.56178105, 0.53916239,\n",
       "        0.55565352, 0.49234157, 0.51152083, 0.56473255, 0.5159115 ,\n",
       "        0.4770821 , 0.48763341, 0.52495142, 0.47221699, 0.55848931,\n",
       "        0.5249264 , 0.52750414, 0.52619588, 0.49708349, 0.53344151,\n",
       "        0.54368357, 0.56517149, 0.48014416, 0.4902168 , 0.53514934,\n",
       "        0.56168575, 0.54227279, 0.54278633, 0.49361413, 0.57237885,\n",
       "        0.47407125, 0.4920552 , 0.53023224, 0.54830349, 0.49476662,\n",
       "        0.49244238, 0.48625256, 0.57408851, 0.49479483, 0.49254633,\n",
       "        0.54068334, 0.55741788, 0.550361  , 0.51917939, 0.5699839 ,\n",
       "        0.56803905, 0.56383964, 0.52172133, 0.47990681, 0.5455173 ,\n",
       "        0.52103228, 0.48148209, 0.47241741, 0.49136636, 0.48625578,\n",
       "        0.56414548, 0.55938433, 0.49602353, 0.53511436, 0.54238234,\n",
       "        0.4889727 , 0.52856691, 0.50390489, 0.48374485, 0.53771206,\n",
       "        0.48617131, 0.5157753 , 0.55477647, 0.51834532, 0.54616046,\n",
       "        0.54572774, 0.49093023, 0.53542207, 0.56493662, 0.55350538],\n",
       "       [0.55239157, 0.54586481, 0.55631227, 0.57969909, 0.58287758,\n",
       "        0.59608117, 0.5891624 , 0.54354383, 0.5122627 , 0.54377143,\n",
       "        0.57142324, 0.51391673, 0.56946741, 0.51122195, 0.5808383 ,\n",
       "        0.55579644, 0.55683277, 0.530111  , 0.59875321, 0.56162791,\n",
       "        0.54015517, 0.60778604, 0.57538115, 0.59010106, 0.58560091,\n",
       "        0.61155139, 0.54309418, 0.53220239, 0.53910398, 0.6072868 ,\n",
       "        0.52557176, 0.59448428, 0.57462793, 0.52417501, 0.52292869,\n",
       "        0.53135321, 0.55560078, 0.59383362, 0.59000248, 0.51217277,\n",
       "        0.53775845, 0.51697192, 0.54825673, 0.60031863, 0.58359401,\n",
       "        0.52463954, 0.51775255, 0.60058589, 0.58412273, 0.59852491,\n",
       "        0.59815545, 0.5471973 , 0.53507133, 0.58936321, 0.58508794,\n",
       "        0.5563926 , 0.57657324, 0.52436169, 0.60798024, 0.5110285 ,\n",
       "        0.55486186, 0.57544663, 0.5616586 , 0.54085885, 0.51767076,\n",
       "        0.51498608, 0.57410843, 0.58944395, 0.59263446, 0.61221471,\n",
       "        0.59790626, 0.57525154, 0.54797544, 0.51227778, 0.54126453,\n",
       "        0.54848042, 0.53709195, 0.52081803, 0.59611397, 0.53521112,\n",
       "        0.59757228, 0.57855121, 0.5200299 , 0.60894257, 0.59238673,\n",
       "        0.52627551, 0.55724237, 0.52252376, 0.52736979, 0.53601234,\n",
       "        0.59671742, 0.52701337, 0.54016213, 0.54158983, 0.58923665,\n",
       "        0.59640981, 0.51867814, 0.51423347, 0.53009424, 0.52584996]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered backward: target = [[1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
      "  1 0 1 1 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1\n",
      "  0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0]\n",
      " [0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0\n",
      "  1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
      "  1 1 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 1]\n",
      " [1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1\n",
      "  0 0 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 1 0 0\n",
      "  0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1]]\n",
      "self.target != None: output\n",
      "Entered backward: target = None\n",
      "self.target == None: hidden\n",
      "Entered backward: target = None\n",
      "input\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.55308483, 0.57060642, 0.57127585, 0.51116767, 0.51429673,\n",
       "        0.58918799, 0.5320979 , 0.52437182, 0.55960736, 0.53453741,\n",
       "        0.52303894, 0.52668929, 0.52469992, 0.50985699, 0.53433008,\n",
       "        0.52463116, 0.58410699, 0.58704386, 0.53038132, 0.52614839,\n",
       "        0.60276981, 0.58672433, 0.60066383, 0.51600162, 0.54748058,\n",
       "        0.58215144, 0.5860736 , 0.52325247, 0.57896977, 0.52504101,\n",
       "        0.57153595, 0.57628479, 0.56737654, 0.53191928, 0.5990363 ,\n",
       "        0.57311031, 0.59301413, 0.58961265, 0.50747044, 0.57886493,\n",
       "        0.54382195, 0.60183189, 0.58733892, 0.54705752, 0.55410878,\n",
       "        0.54019972, 0.51146949, 0.57068685, 0.51167995, 0.58144989,\n",
       "        0.53680763, 0.55732031, 0.54038827, 0.52528295, 0.59493734,\n",
       "        0.56915354, 0.57106836, 0.50627798, 0.56302076, 0.59119437,\n",
       "        0.57284544, 0.51462158, 0.54218939, 0.51170172, 0.56226095,\n",
       "        0.60202234, 0.51787829, 0.5309385 , 0.54878633, 0.55744983,\n",
       "        0.60680293, 0.53240159, 0.59568601, 0.51172813, 0.58976035,\n",
       "        0.52568895, 0.56511393, 0.50511471, 0.58883373, 0.55510401,\n",
       "        0.55973194, 0.57970759, 0.51237465, 0.56802848, 0.5815896 ,\n",
       "        0.60130245, 0.50763096, 0.58611255, 0.59718717, 0.5488155 ,\n",
       "        0.55244577, 0.54646379, 0.51782068, 0.57884908, 0.5638776 ,\n",
       "        0.50814385, 0.58711176, 0.5309187 , 0.59795402, 0.55845153],\n",
       "       [0.52657221, 0.53500188, 0.51328728, 0.49530477, 0.5406888 ,\n",
       "        0.52003072, 0.55383805, 0.48907033, 0.48572648, 0.5732396 ,\n",
       "        0.54811468, 0.54065889, 0.48284001, 0.52617236, 0.47634725,\n",
       "        0.5057189 , 0.55015004, 0.51106524, 0.52614458, 0.51739419,\n",
       "        0.53177554, 0.50429904, 0.54428501, 0.47483879, 0.54218597,\n",
       "        0.49345546, 0.5612088 , 0.54413167, 0.56178105, 0.53916239,\n",
       "        0.55565352, 0.49234157, 0.51152083, 0.56473255, 0.5159115 ,\n",
       "        0.4770821 , 0.48763341, 0.52495142, 0.47221699, 0.55848931,\n",
       "        0.5249264 , 0.52750414, 0.52619588, 0.49708349, 0.53344151,\n",
       "        0.54368357, 0.56517149, 0.48014416, 0.4902168 , 0.53514934,\n",
       "        0.56168575, 0.54227279, 0.54278633, 0.49361413, 0.57237885,\n",
       "        0.47407125, 0.4920552 , 0.53023224, 0.54830349, 0.49476662,\n",
       "        0.49244238, 0.48625256, 0.57408851, 0.49479483, 0.49254633,\n",
       "        0.54068334, 0.55741788, 0.550361  , 0.51917939, 0.5699839 ,\n",
       "        0.56803905, 0.56383964, 0.52172133, 0.47990681, 0.5455173 ,\n",
       "        0.52103228, 0.48148209, 0.47241741, 0.49136636, 0.48625578,\n",
       "        0.56414548, 0.55938433, 0.49602353, 0.53511436, 0.54238234,\n",
       "        0.4889727 , 0.52856691, 0.50390489, 0.48374485, 0.53771206,\n",
       "        0.48617131, 0.5157753 , 0.55477647, 0.51834532, 0.54616046,\n",
       "        0.54572774, 0.49093023, 0.53542207, 0.56493662, 0.55350538],\n",
       "       [0.55239157, 0.54586481, 0.55631227, 0.57969909, 0.58287758,\n",
       "        0.59608117, 0.5891624 , 0.54354383, 0.5122627 , 0.54377143,\n",
       "        0.57142324, 0.51391673, 0.56946741, 0.51122195, 0.5808383 ,\n",
       "        0.55579644, 0.55683277, 0.530111  , 0.59875321, 0.56162791,\n",
       "        0.54015517, 0.60778604, 0.57538115, 0.59010106, 0.58560091,\n",
       "        0.61155139, 0.54309418, 0.53220239, 0.53910398, 0.6072868 ,\n",
       "        0.52557176, 0.59448428, 0.57462793, 0.52417501, 0.52292869,\n",
       "        0.53135321, 0.55560078, 0.59383362, 0.59000248, 0.51217277,\n",
       "        0.53775845, 0.51697192, 0.54825673, 0.60031863, 0.58359401,\n",
       "        0.52463954, 0.51775255, 0.60058589, 0.58412273, 0.59852491,\n",
       "        0.59815545, 0.5471973 , 0.53507133, 0.58936321, 0.58508794,\n",
       "        0.5563926 , 0.57657324, 0.52436169, 0.60798024, 0.5110285 ,\n",
       "        0.55486186, 0.57544663, 0.5616586 , 0.54085885, 0.51767076,\n",
       "        0.51498608, 0.57410843, 0.58944395, 0.59263446, 0.61221471,\n",
       "        0.59790626, 0.57525154, 0.54797544, 0.51227778, 0.54126453,\n",
       "        0.54848042, 0.53709195, 0.52081803, 0.59611397, 0.53521112,\n",
       "        0.59757228, 0.57855121, 0.5200299 , 0.60894257, 0.59238673,\n",
       "        0.52627551, 0.55724237, 0.52252376, 0.52736979, 0.53601234,\n",
       "        0.59671742, 0.52701337, 0.54016213, 0.54158983, 0.58923665,\n",
       "        0.59640981, 0.51867814, 0.51423347, 0.53009424, 0.52584996]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.backward(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
