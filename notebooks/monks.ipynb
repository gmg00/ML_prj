{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    df = pd.read_csv(filename, names = ['all'])\n",
    "    df['all'] = df['all'].str.strip()\n",
    "    df['all1']  = df['all'].apply(lambda x: np.array(x.split(' ')))\n",
    "    df[['target','attr1','attr2','attr3','attr4','attr5','attr6','id']] = pd.DataFrame(df.all1.tolist(), index= df.index)\n",
    "    return df.drop(columns=['all','all1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('/Users/HP/Desktop/UNI/LM_1/MachineLearning/ML_prj/data/MONK/monks-1.train')\n",
    "df_test = get_data('/Users/HP/Desktop/UNI/LM_1/MachineLearning/ML_prj/data/MONK/monks-1.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: ['1' '0']\n",
      "attr1: ['1' '2' '3']\n",
      "attr2: ['1' '2' '3']\n",
      "attr3: ['1' '2']\n",
      "attr4: ['1' '3' '2']\n",
      "attr5: ['3' '2' '4' '1']\n",
      "attr6: ['1' '2']\n"
     ]
    }
   ],
   "source": [
    "for name in df.columns[:-1]:\n",
    "    print(f'{name}: {df[name].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr6</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>data_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data_27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>data_416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>data_426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>data_428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>data_432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    target attr1 attr2 attr3 attr4 attr5 attr6        id\n",
       "0        1     1     1     1     1     3     1    data_5\n",
       "1        1     1     1     1     1     3     2    data_6\n",
       "2        1     1     1     1     3     2     1   data_19\n",
       "3        1     1     1     1     3     3     2   data_22\n",
       "4        1     1     1     2     1     2     1   data_27\n",
       "..     ...   ...   ...   ...   ...   ...   ...       ...\n",
       "119      1     3     3     2     1     4     2  data_416\n",
       "120      1     3     3     2     3     1     2  data_426\n",
       "121      1     3     3     2     3     2     2  data_428\n",
       "122      1     3     3     2     3     3     2  data_430\n",
       "123      1     3     3     2     3     4     2  data_432\n",
       "\n",
       "[124 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>attr1</th>\n",
       "      <th>attr2</th>\n",
       "      <th>attr3</th>\n",
       "      <th>attr4</th>\n",
       "      <th>attr5</th>\n",
       "      <th>attr6</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>data_337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>data_273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>data_42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>data_86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>data_135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>data_132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>data_181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>data_196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data_67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>data_137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    target attr1 attr2 attr3 attr4 attr5 attr6        id\n",
       "0        1     3     2     1     1     1     1  data_337\n",
       "1        1     2     3     2     2     1     1  data_273\n",
       "2        1     1     1     2     3     1     2   data_42\n",
       "3        0     1     2     2     2     3     2   data_86\n",
       "4        0     1     3     2     2     4     1  data_135\n",
       "..     ...   ...   ...   ...   ...   ...   ...       ...\n",
       "119      0     1     3     2     2     2     2  data_132\n",
       "120      0     2     1     2     2     3     1  data_181\n",
       "121      1     2     2     1     1     2     2  data_196\n",
       "122      0     1     2     1     3     2     1   data_67\n",
       "123      1     1     3     2     3     1     1  data_137\n",
       "\n",
       "[124 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df.drop(columns=['target','id']), df['target'].apply(lambda x: int(x))\n",
    "X_test, y_test = df_test.drop(columns=['target','id']), df_test['target'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    62\n",
       "0    62\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X_train, y_train)\n",
    "X_train = ohe.transform(X_train).toarray()\n",
    "X_test = ohe.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 17)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 54        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 58 (232.00 Byte)\n",
      "Trainable params: 58 (232.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "2/2 [==============================] - 3s 475ms/step - loss: 3.4860 - val_loss: 1.8033 - lr: 0.0050\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.8272 - val_loss: 1.5034 - lr: 0.0050\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.5461 - val_loss: 1.3391 - lr: 0.0050\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 1.3967 - val_loss: 1.2233 - lr: 0.0050\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 1.2747 - val_loss: 1.1359 - lr: 0.0050\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 1.1834 - val_loss: 1.0665 - lr: 0.0050\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.1129 - val_loss: 1.0096 - lr: 0.0050\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.0538 - val_loss: 0.9623 - lr: 0.0050\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 1.0036 - val_loss: 0.9225 - lr: 0.0050\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.9624 - val_loss: 0.8887 - lr: 0.0050\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.9273 - val_loss: 0.8599 - lr: 0.0050\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.8942 - val_loss: 0.8354 - lr: 0.0050\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.8686 - val_loss: 0.8143 - lr: 0.0050\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.8448 - val_loss: 0.7962 - lr: 0.0050\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.8244 - val_loss: 0.7806 - lr: 0.0050\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.8076 - val_loss: 0.7670 - lr: 0.0050\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.7943 - val_loss: 0.7552 - lr: 0.0050\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.7811 - val_loss: 0.7449 - lr: 0.0050\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.7683 - val_loss: 0.7362 - lr: 0.0050\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.7571 - val_loss: 0.7287 - lr: 0.0050\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.7487 - val_loss: 0.7221 - lr: 0.0050\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 211ms/step - loss: 0.7399 - val_loss: 0.7164 - lr: 0.0050\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.7325 - val_loss: 0.7115 - lr: 0.0050\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7263 - val_loss: 0.7072 - lr: 0.0050\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 0.7213 - val_loss: 0.7034 - lr: 0.0050\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.7151 - val_loss: 0.7002 - lr: 0.0050\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7111 - val_loss: 0.6973 - lr: 0.0050\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 0.7067 - val_loss: 0.6947 - lr: 0.0050\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 222ms/step - loss: 0.7020 - val_loss: 0.6925 - lr: 0.0050\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6989 - val_loss: 0.6905 - lr: 0.0050\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6957 - val_loss: 0.6887 - lr: 0.0050\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.6929 - val_loss: 0.6871 - lr: 0.0050\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6898 - val_loss: 0.6856 - lr: 0.0050\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6871 - val_loss: 0.6843 - lr: 0.0050\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.6847 - val_loss: 0.6831 - lr: 0.0050\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6818 - val_loss: 0.6819 - lr: 0.0050\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.6797 - val_loss: 0.6809 - lr: 0.0050\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 262ms/step - loss: 0.6771 - val_loss: 0.6798 - lr: 0.0050\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 246ms/step - loss: 0.6749 - val_loss: 0.6789 - lr: 0.0050\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.6726 - val_loss: 0.6779 - lr: 0.0050\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 246ms/step - loss: 0.6700 - val_loss: 0.6770 - lr: 0.0050\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 0.6681 - val_loss: 0.6761 - lr: 0.0050\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.6656 - val_loss: 0.6752 - lr: 0.0050\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.6629 - val_loss: 0.6743 - lr: 0.0050\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.6609 - val_loss: 0.6734 - lr: 0.0050\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.6582 - val_loss: 0.6725 - lr: 0.0050\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6556 - val_loss: 0.6716 - lr: 0.0050\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.6530 - val_loss: 0.6706 - lr: 0.0050\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.6504 - val_loss: 0.6696 - lr: 0.0050\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.6475 - val_loss: 0.6686 - lr: 0.0050\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.6445 - val_loss: 0.6676 - lr: 0.0050\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.6414 - val_loss: 0.6665 - lr: 0.0050\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.6386 - val_loss: 0.6654 - lr: 0.0050\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.6351 - val_loss: 0.6643 - lr: 0.0050\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.6316 - val_loss: 0.6632 - lr: 0.0050\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.6281 - val_loss: 0.6620 - lr: 0.0050\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 260ms/step - loss: 0.6244 - val_loss: 0.6609 - lr: 0.0050\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.6206 - val_loss: 0.6597 - lr: 0.0050\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6168 - val_loss: 0.6585 - lr: 0.0050\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.6129 - val_loss: 0.6573 - lr: 0.0050\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.6081 - val_loss: 0.6562 - lr: 0.0050\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.6041 - val_loss: 0.6551 - lr: 0.0050\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 203ms/step - loss: 0.5996 - val_loss: 0.6540 - lr: 0.0050\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.5945 - val_loss: 0.6531 - lr: 0.0050\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.5895 - val_loss: 0.6522 - lr: 0.0050\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.5849 - val_loss: 0.6515 - lr: 0.0050\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.5797 - val_loss: 0.6509 - lr: 0.0050\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 0.5746 - val_loss: 0.6504 - lr: 0.0050\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 0.5697 - val_loss: 0.6502 - lr: 0.0050\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 0.5645 - val_loss: 0.6501 - lr: 0.0050\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 260ms/step - loss: 0.5598 - val_loss: 0.6505 - lr: 0.0050\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 247ms/step - loss: 0.5536 - val_loss: 0.6510 - lr: 0.0050\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.5485 - val_loss: 0.6518 - lr: 0.0050\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5436 - val_loss: 0.6530 - lr: 0.0050\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5381 - val_loss: 0.6546 - lr: 0.0050\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.5332 - val_loss: 0.6568 - lr: 0.0050\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.5288 - val_loss: 0.6595 - lr: 0.0050\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5234 - val_loss: 0.6630 - lr: 0.0050\n",
      "Epoch 79/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5164\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.5188 - val_loss: 0.6669 - lr: 0.0050\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5152 - val_loss: 0.6681 - lr: 0.0012\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.5142 - val_loss: 0.6688 - lr: 0.0012\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.5132 - val_loss: 0.6695 - lr: 0.0012\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.5123 - val_loss: 0.6702 - lr: 0.0012\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.5113 - val_loss: 0.6706 - lr: 0.0012\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.5104 - val_loss: 0.6708 - lr: 0.0012\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.5096 - val_loss: 0.6708 - lr: 0.0012\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.5085 - val_loss: 0.6709 - lr: 0.0012\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.5077 - val_loss: 0.6708 - lr: 0.0012\n",
      "Epoch 89/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5513\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "2/2 [==============================] - 0s 211ms/step - loss: 0.5069 - val_loss: 0.6705 - lr: 0.0012\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5060 - val_loss: 0.6705 - lr: 3.1250e-04\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.5057 - val_loss: 0.6705 - lr: 3.1250e-04\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.5055 - val_loss: 0.6705 - lr: 3.1250e-04\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.5053 - val_loss: 0.6705 - lr: 3.1250e-04\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5050 - val_loss: 0.6704 - lr: 3.1250e-04\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.5048 - val_loss: 0.6704 - lr: 3.1250e-04\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5046 - val_loss: 0.6704 - lr: 3.1250e-04\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5043 - val_loss: 0.6704 - lr: 3.1250e-04\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.5041 - val_loss: 0.6703 - lr: 3.1250e-04\n",
      "Epoch 99/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5686\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5039 - val_loss: 0.6703 - lr: 3.1250e-04\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.5037 - val_loss: 0.6703 - lr: 7.8125e-05\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5037 - val_loss: 0.6703 - lr: 7.8125e-05\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5036 - val_loss: 0.6703 - lr: 7.8125e-05\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5035 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5035 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.5034 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 106/1000\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5034 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 107/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5033 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 108/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5033 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 109/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.5160\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5032 - val_loss: 0.6702 - lr: 7.8125e-05\n",
      "Epoch 110/1000\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5032 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 111/1000\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5031 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 112/1000\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5031 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 113/1000\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5031 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 114/1000\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.5031 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 115/1000\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5031 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 116/1000\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5031 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 117/1000\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.5030 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 118/1000\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.5030 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 119/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4615\n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 0.5030 - val_loss: 0.6702 - lr: 1.9531e-05\n",
      "Epoch 120/1000\n",
      "2/2 [==============================] - 0s 199ms/step - loss: 0.5030 - val_loss: 0.6702 - lr: 4.8828e-06\n",
      "Epoch 120: early stopping\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(17,))\n",
    "\n",
    "hidden = Dense(3,activation='tanh',kernel_initializer='random_normal',bias_initializer='zeros')(input_data)\n",
    "\n",
    "#Output.\n",
    "outputs = Dense(1, activation='tanh',kernel_initializer='random_normal',bias_initializer='zeros')(hidden)\n",
    "\n",
    "\n",
    "model = Model(inputs=input_data, outputs=outputs)\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=5e-3))\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.5, epochs = 1000,\n",
    "                    callbacks = [EarlyStopping(monitor='val_loss',\n",
    "                                   patience=50,\n",
    "                                   verbose=1),\n",
    "                                ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.25,\n",
    "                                       patience=10,\n",
    "                                       verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss,'-')\n",
    "plt.plot(val_loss,'--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soglia = 0 \n",
    "pred[pred <= soglia] = 0\n",
    "pred[pred > soglia] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sol = pd.DataFrame()\n",
    "df_sol['true'] = y_test\n",
    "df_sol['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     223\n",
       "False    209\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_sol['true'] == df_sol['pred']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.T\n",
    "y_train = y_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 124)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "#linear activation function \n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def d_linear(x):\n",
    "    return 1\n",
    "\n",
    "#sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    f =sigmoid(x)\n",
    "    return f * (1-f)\n",
    "\n",
    "#ReLu activation function\n",
    "def relu(x):\n",
    "    if x > 0: return x\n",
    "    else: return 0\n",
    "\n",
    "def d_relu(x):\n",
    "    if x > 0: return 1\n",
    "    else: return 0\n",
    "\n",
    "def MSE(layer, target):\n",
    "    return (layer - target)/target.shape[1]\n",
    "\n",
    "#def binary_crossentropy(layer, target):\n",
    "#    return (layer - target)/(layer*(1-layer)*target.shape[1])\n",
    "\n",
    "def binary_crossentropy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calcola la derivata della binary crossentropy rispetto ai valori predetti.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Valori veri (etichette reali).\n",
    "    - y_pred: Valori predetti (output del modello).\n",
    "\n",
    "    Returns:\n",
    "    - La derivata della binary crossentropy rispetto ai valori predetti.\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Calcolo della derivata rispetto a y_pred\n",
    "    derivative = - (y_true / y_pred - (1 - y_true) / (1 - y_pred))\n",
    "\n",
    "    return derivative / len(y_true)  # Media su tutti gli esempi di addestramento\n",
    "    \n",
    "\n",
    "#hyperbolic tangent activation function\n",
    "def TanH(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_TanH(x):\n",
    "    return 1 - np.tanh(x)**2 \n",
    "\n",
    "#def softmax(x):\n",
    "#    return np.exp(-x)/(np.exp(-x)+np.exp(x-1))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Calcola la funzione Softmax per un array di input.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Un array di input.\n",
    "\n",
    "    Returns:\n",
    "    - Un array con valori Softmax normalizzati.\n",
    "    \"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # Evita l'overflow sottraendo il massimo valore\n",
    "    return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "#def d_softmax(x):\n",
    "#    return x*(x-1)\n",
    "\n",
    "def d_softmax(x):\n",
    "    \"\"\"\n",
    "    Calcola la derivata della funzione Softmax rispetto agli elementi di input.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Un array di input.\n",
    "\n",
    "    Returns:\n",
    "    - Un array con i valori della derivata Softmax.\n",
    "    \"\"\"\n",
    "    softmax_output = softmax(x)\n",
    "    return softmax_output * (1 - softmax_output)\n",
    "\n",
    "#Dictionary for the activation functions\n",
    "act_func = {\n",
    "    'lin': linear,\n",
    "    'sigm': sigmoid,\n",
    "    'relu': relu,\n",
    "    'tanh': TanH,\n",
    "    'softmax' : softmax\n",
    "}\n",
    "\n",
    "#A second dictionary for their derivatives\n",
    "d_act_func = {\n",
    "    'lin': d_linear,\n",
    "    'sigm': d_sigmoid,\n",
    "    'relu': d_relu,\n",
    "    'tanh': d_TanH,\n",
    "    'softmax': d_softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input, prev_layer, dim_layer, act_function, target):\n",
    "\n",
    "        self.dim_layer = dim_layer\n",
    "        self.prev_layer = prev_layer\n",
    "        self.act_function = np.vectorize(act_func[act_function])\n",
    "        self.d_act_function = np.vectorize(d_act_func[act_function])\n",
    "        self.target = target\n",
    "\n",
    "        if self.prev_layer != None:\n",
    "            self.dim_batch = self.prev_layer.dim_batch\n",
    "            self.W = np.random.uniform(-0.5, 0.5, (self.dim_layer, self.prev_layer.dim_layer))    #inizializzo la matrice dei pesi\n",
    "            self.b = np.random.uniform(-0.5, 0.5, (self.dim_layer, 1))      #inizializzo il vettore dei bias\n",
    "            self.layer = np.empty((self.dim_layer, self.dim_batch))\n",
    "        else: \n",
    "            self.layer = input\n",
    "            self.dim_batch = input.shape[1]\n",
    "\n",
    "    def forward(self):\n",
    "        if self.prev_layer == None:\n",
    "            #print('prev_layer == None: return self.layer')\n",
    "            #print(self.layer.shape)\n",
    "            return self.layer\n",
    "        else: \n",
    "            #print('prev_layer =! None: compute forward')\n",
    "            #print(f'W = {self.W}')\n",
    "            #print(f'b = {self.b}')\n",
    "            self.z = self.W.dot(self.prev_layer.forward()) + self.b\n",
    "            self.layer = self.act_function(self.z)\n",
    "            #print(f'layer = {self.layer}')\n",
    "            return self.layer\n",
    "    \n",
    "    def backward(self, next_delta = None, next_weights = None):\n",
    "        #print(f'Entered backward: target = {self.target}')\n",
    "        \n",
    "        if self.target is None:\n",
    "            if self.prev_layer != None:\n",
    "                #print('self.target == None: hidden')\n",
    "                delta = self.d_act_function(self.z) * next_weights.T.dot(next_delta)\n",
    "                #self.prev_layer.backward(delta,self.weights)\n",
    "                self.d_W = delta.dot(self.prev_layer.backward(delta,self.W).T)\n",
    "                self.d_b = delta.sum(axis=1).reshape((delta.shape[0],1))\n",
    "                return self.layer\n",
    "\n",
    "            else: \n",
    "                #print('input')\n",
    "                return self.layer\n",
    "            \n",
    "        else:\n",
    "            #print('self.target != None: output')\n",
    "            delta = self.d_act_function(self.z) * binary_crossentropy(self.layer, self.target)\n",
    "            #self.prev_layer.backward(delta,self.weights)\n",
    "            self.d_W = delta.dot(self.prev_layer.backward(delta,self.W).T)\n",
    "            self.d_b = delta.sum(axis=1).reshape((delta.shape[0],1))\n",
    "            return self.layer    \n",
    "\n",
    "    def update_weights(self, eta, lam):\n",
    "        if self.prev_layer is None:\n",
    "            return\n",
    "        else:  \n",
    "            self.W = self.W - eta * self.d_W - lam * self.W\n",
    "            self.prev_layer.update_weights(eta, lam)\n",
    "\n",
    "    def err(self):\n",
    "        #return - (self.target * np.log(self.layer) + (1 - self.target) * np.log(1 - self.layer)).mean()\n",
    "        return np.sqrt((self.layer-self.target)**2).mean()\n",
    "    \n",
    "    def rel_err(self):\n",
    "        return np.sqrt((self.layer[0]-self.target[0])**2/self.target[0]**2+(self.layer[1]-self.target[1])**2/self.target[1]**2+(self.layer[2]-self.target[2])**2/self.target[2]**2).mean()\n",
    "\n",
    "    def err_i(self,i):\n",
    "        return np.sqrt((self.layer[i]-self.target[i])**2).mean()\n",
    "    \n",
    "    def rel_err_i(self,i):\n",
    "        return (np.sqrt((self.layer[i]-self.target[i])**2)/self.target[i]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy().reshape((1,124))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "lam = 0.00\n",
    "o = 0\n",
    "E1 = []\n",
    "E2 = []\n",
    "E3 = []\n",
    "E = []\n",
    "dim_batch = 100\n",
    "input_layer = Layer(X_train, None, 17, 'lin', None)\n",
    "hidden_layer = Layer(None, input_layer, 5, 'relu', None)\n",
    "output_layer = Layer(None, hidden_layer, 1, 'sigm', np.array(y_train).reshape((1,124)))\n",
    "while o<1000:\n",
    "    \n",
    "    output_layer.forward()\n",
    "\n",
    "    output_layer.backward()\n",
    "    output_layer.update_weights(eta, lam)\n",
    "    if o % 10 == 0: print(o)\n",
    "\n",
    "\n",
    "    E.append(output_layer.err())\n",
    "    o += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  [y_train, -(y_train-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_train).reshape((2,124))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49930702345103534,\n",
       " 0.4902519167723594,\n",
       " 0.4747403359492758,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.array(E).shape)\n",
    "plt.plot(E)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o = 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layer_dim)):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m         layers_z[l] \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m \u001b[38;5;241m+\u001b[39m biases[l]\n\u001b[0;32m     79\u001b[0m         layers_a[l] \u001b[38;5;241m=\u001b[39m act_functions[l](layers_z[l])\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def linear(x):\n",
    "    '''Linear activation function'''\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid activation function'''\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def d_linear(x):\n",
    "    return 1\n",
    "\n",
    "#ReLu activation function\n",
    "def relu(x):\n",
    "    if x > 0: return x\n",
    "    else: return 0\n",
    "\n",
    "def d_relu(x):\n",
    "    if x > 0: return 1\n",
    "    else: return 0\n",
    "\n",
    "relu = np.vectorize(relu)\n",
    "d_relu = np.vectorize(d_relu)\n",
    "\n",
    "layers_a = []\n",
    "layers_z = []\n",
    "weights = []\n",
    "biases = []\n",
    "\n",
    "d_weights = []\n",
    "d_biases = []\n",
    "\n",
    "N = 1000 #batch dimension\n",
    "input_dim = 17\n",
    "hidden_dim = 8\n",
    "output_dim = 2\n",
    "layer_dim = [hidden_dim, output_dim]\n",
    "\n",
    "for i in range(len(layer_dim)):\n",
    "    layers_a.append(np.empty((layer_dim[i],N)))\n",
    "    layers_z.append(np.empty((layer_dim[i],N)))\n",
    "\n",
    "#normalisation\n",
    "X_train_norm = (X_train - np.min(X_train, axis=0))/(np.max(X_train, axis=0) - np.min(X_train, axis=0))\n",
    "y_norm = (np.array(y_train).reshape((2,124)) - np.min(np.array(y_train).reshape((2,124)), axis=0))/(np.max(np.array(y_train).reshape((2,124)),axis=0)- np.min(np.array(y_train).reshape((2,124)), axis=0))\n",
    "\n",
    "#input = X_train_norm.T\n",
    "#y = y_norm.T\n",
    "\n",
    "w_ji = np.random.uniform(-0.5,0.5,(hidden_dim,input_dim))\n",
    "weights.append(w_ji)\n",
    "d_weights.append(np.empty((hidden_dim,input_dim)))\n",
    "w_kj = np.random.uniform(-0.5,0.5,(output_dim,hidden_dim))\n",
    "weights.append(w_kj)\n",
    "d_weights.append(np.empty((output_dim,hidden_dim)))\n",
    "\n",
    "b_j = np.ones((hidden_dim,1))\n",
    "biases.append(b_j)\n",
    "d_biases.append(np.empty((hidden_dim,1)))\n",
    "b_k = np.ones((output_dim,1))\n",
    "biases.append(b_k)\n",
    "d_biases.append(np.empty((output_dim,1)))\n",
    "\n",
    "act_functions = [relu, softmax]\n",
    "d_act_functions = [d_relu, d_softmax]\n",
    "o = 0\n",
    "E = []\n",
    "deltas = []\n",
    "deltas.append(np.empty((hidden_dim,N)))\n",
    "deltas.append(np.empty((output_dim,N)))\n",
    "while o<1000:\n",
    "    print(f'o = {o}')\n",
    "    #forward propagation\n",
    "    for l in range(len(layer_dim)):\n",
    "        if l == 0:\n",
    "            layers_z[l] = weights[l] @ input + biases[l]\n",
    "            layers_a[l] = act_functions[l](layers_z[l])\n",
    "        else:\n",
    "            layers_z[l] = weights[l] @ layers_a[l-1] + biases[l]\n",
    "            layers_a[l] = act_functions[l](layers_z[l])\n",
    "\n",
    "    #error calculation\n",
    "    e = np.sqrt((layers_a[-1][0]-y[0])**2+(layers_a[-1][1]-y[1])**2+(layers_a[-1][2]-y[2])**2).mean()\n",
    "    E.append(e)\n",
    "\n",
    "    #backpropagation\n",
    "    for l in range(len(layer_dim)-1,-1,-1):\n",
    "        if l == len(layer_dim)-1:\n",
    "            tmp = binary_crossentropy(layers_a[l]-np.array(y_train).reshape((2,124)))\n",
    "            deltas[l] = d_act_functions[l](layers_z[l])*tmp\n",
    "        else:\n",
    "            deltas[l] = d_act_functions[l](layers_z[l]) * (weights[l+1].T @ deltas[l+1])\n",
    "\n",
    "    for i in range(len(d_weights)):\n",
    "        if i == 0:\n",
    "            d_weights[i] = deltas[i] @ input.T\n",
    "            d_biases[i] = deltas[i].sum(axis=1).reshape((deltas[i].shape[0],1))\n",
    "        else:\n",
    "            d_weights[i] = deltas[i] @ layers_a[i-1].T\n",
    "            d_biases[i] = deltas[i].sum(axis=1).reshape((deltas[i].shape[0],1))\n",
    "\n",
    "    #print(d_weights[0])\n",
    "    #print(d_weights[1].shape)\n",
    "    #print(d_biases[0].shape)\n",
    "    #print(d_biases[1].shape)\n",
    "    #weight update\n",
    "    eta = 0.01\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] = weights[i] - d_weights[i]*eta\n",
    "        biases[i] = biases[i] - d_biases[i]*eta\n",
    "\n",
    "    o += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 124)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
