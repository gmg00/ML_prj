{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "from functions import *\n",
    "from Layer import Layer, Input\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('/Users/HP/Desktop/UNI/LM_1/MachineLearning/ML_prj/data/MONK/monks-1.train')\n",
    "df_test = get_data('/Users/HP/Desktop/UNI/LM_1/MachineLearning/ML_prj/data/MONK/monks-1.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = get_data(r\"\\Users\\s512fj-ej021t\\OneDrive\\Desktop\\ML\\monks-1.train\")\n",
    "#df_test = get_data(r\"\\Users\\s512fj-ej021t\\OneDrive\\Desktop\\ML\\monks-1.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df.drop(columns=['target','id']).to_numpy().T, df['target'].apply(lambda x: int(x)).to_numpy().T\n",
    "X_test, y_test = df_test.drop(columns=['target','id']).to_numpy().T, df_test['target'].apply(lambda x: int(x)).to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = onehot_encoding(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(input, target, folds, metrics, params, callbacks):\n",
    "\n",
    "    dim_input = input.shape[1]\n",
    "    input_index = np.arange(dim_input)\n",
    "    np.random.shuffle(input_index)\n",
    "    subset_index = []\n",
    "    \n",
    "    for i in range(folds-1):\n",
    "        subset_index.append(input_index[i*np.round(dim_input / folds).astype(int): (i+1)*np.round(dim_input / folds).astype(int)])\n",
    "    subset_index.append(input_index[(i+1)*np.round(dim_input / folds).astype(int):])\n",
    "\n",
    "   \n",
    "    history_cv = {'train_loss': [],\n",
    "                  #'train_loss_var': [],\n",
    "                  'val_loss': []\n",
    "                  #'val_loss_var': []\n",
    "                  }\n",
    "    \n",
    "    for m in metrics:\n",
    "        history_cv[f'train_{m.__name__}'] = []\n",
    "        #history_cv[f'train_{m.__name__}_var'] = []\n",
    "        history_cv[f'val_{m.__name__}'] = []\n",
    "        #history_cv[f'val_{m.__name__}_var'] = []\n",
    "\n",
    "    for val_ind in subset_index:\n",
    "\n",
    "        train_ind = list(set(input_index) - set(val_ind))\n",
    "\n",
    "        train_input = input[:,train_ind]\n",
    "        train_target = target[:,train_ind]\n",
    "        val_input = input[:,val_ind]\n",
    "        val_target = target[:,val_ind]\n",
    "\n",
    "        input_layer = Input(17)\n",
    "        hidden_layer = Layer(input_layer, 15, 'relu')\n",
    "        output_layer = Layer(hidden_layer, 1, 'sigm')\n",
    "\n",
    "        model = NeuralNetwork(input_layer, output_layer, loss = 'binary_crossentropy', metrics = metrics)\n",
    "\n",
    "        history = model.train(train_input, train_target, **params,\n",
    "                              **callbacks,\n",
    "                                validation_data = [val_input, val_target],\n",
    "                                verbose=0\n",
    "                            )\n",
    "        \n",
    "        history_cv['train_loss'].append(history['train_loss'][-1])\n",
    "        history_cv['val_loss'].append(history['val_loss'][-1])\n",
    "        for m in metrics:\n",
    "            history_cv[f'train_{m.__name__}'].append(history[f'train_{m.__name__}'][-1])\n",
    "            history_cv[f'val_{m.__name__}'].append(history[f'val_{m.__name__}'][-1])\n",
    "    \n",
    "    history_cv['train_loss_mean'] = np.mean(history_cv['train_loss'])\n",
    "    history_cv['train_loss_std'] = np.std(history_cv['train_loss'])\n",
    "    history_cv['val_loss_mean'] = np.mean(history_cv['val_loss'])\n",
    "    history_cv['val_loss_std'] = np.std(history_cv['val_loss'])\n",
    "    del history_cv['train_loss']\n",
    "    del history_cv['val_loss']\n",
    "\n",
    "    for m in metrics:\n",
    "        history_cv[f'train_{m.__name__}_mean'] = np.mean(history_cv[f'train_{m.__name__}'])\n",
    "        history_cv[f'train_{m.__name__}_std'] = np.std(history_cv[f'train_{m.__name__}'])\n",
    "        history_cv[f'val_{m.__name__}_mean'] = np.mean(history_cv[f'val_{m.__name__}'])\n",
    "        history_cv[f'val_{m.__name__}_std'] = np.std(history_cv[f'val_{m.__name__}'])\n",
    "        del history_cv[f'train_{m.__name__}']\n",
    "        del history_cv[f'val_{m.__name__}']\n",
    "\n",
    "    return history_cv\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(input, target, params, cv_folds, metrics, callbacks):\n",
    "    param_grid = makeGrid(params)\n",
    "    print('Starting grid_search...')\n",
    "    print(f'Grid of parameters: {params}')\n",
    "    print('-------------------------------------------------')\n",
    "    for p_comb in param_grid:\n",
    "        print(f'Starting params: {p_comb}')\n",
    "        p_comb['results'] = cross_validation(input, target, cv_folds, metrics, p_comb, callbacks)\n",
    "        \n",
    "        print(f'Results:')\n",
    "        for key, value in p_comb['results'].items():\n",
    "            print(f'{key}: {value:.2e}')\n",
    "        print('-------------------------------------------------')\n",
    "    \n",
    "    best_m = param_grid[0]['results'][f'val_accuracy_mean']\n",
    "    best_comb = param_grid[0]\n",
    "    for p_comb in param_grid:\n",
    "        if p_comb['results'][f'val_accuracy_mean'] > best_m:\n",
    "            best_m = p_comb['results'][f'val_accuracy_mean']\n",
    "            best_comb = p_comb\n",
    "    print(f'Best combination of parameters: {best_comb}')\n",
    "    return best_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'eta' : [0.01,0.02,0.03],\n",
    "          'lam' : [0.0, 0.1],\n",
    "          'epochs': [500],\n",
    "          'n_batch' : [31]\n",
    "          #alpha\n",
    "          #dim hidden\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = {'patience' : 150,\n",
    "                  'monitor' : 'val_accuracy',\n",
    "                  'verbose' : 0,\n",
    "                  'compare_function': np.greater_equal}\n",
    "\n",
    "reduce_eta = {'patience' : 75,\n",
    "              'monitor' : 'val_accuracy',\n",
    "              'factor' : 0.5,\n",
    "              'verbose' : 0,\n",
    "              'compare_function': np.greater_equal}\n",
    "\n",
    "callbacks = {'early_stopping': None,\n",
    "             'reduce_eta': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid_search...\n",
      "Grid of parameters: {'eta': [0.01, 0.02, 0.03], 'lam': [0.0, 0.1], 'epochs': [500], 'n_batch': [31]}\n",
      "-------------------------------------------------\n",
      "Starting params: {'eta': 0.01, 'lam': 0.0, 'epochs': 500, 'n_batch': 31}\n",
      "Results:\n",
      "train_loss_mean: 1.04e-03\n",
      "train_loss_std: 1.70e-04\n",
      "val_loss_mean: 2.57e-03\n",
      "val_loss_std: 5.04e-04\n",
      "train_accuracy_mean: 1.00e+02\n",
      "train_accuracy_std: 0.00e+00\n",
      "val_accuracy_mean: 1.00e+02\n",
      "val_accuracy_std: 0.00e+00\n",
      "-------------------------------------------------\n",
      "Starting params: {'eta': 0.01, 'lam': 0.1, 'epochs': 500, 'n_batch': 31}\n",
      "Results:\n",
      "train_loss_mean: 7.55e-03\n",
      "train_loss_std: 5.96e-04\n",
      "val_loss_mean: 1.51e-02\n",
      "val_loss_std: 6.24e-03\n",
      "train_accuracy_mean: 9.90e+01\n",
      "train_accuracy_std: 1.25e+00\n",
      "val_accuracy_mean: 9.28e+01\n",
      "val_accuracy_std: 8.91e+00\n",
      "-------------------------------------------------\n",
      "Starting params: {'eta': 0.02, 'lam': 0.0, 'epochs': 500, 'n_batch': 31}\n",
      "Results:\n",
      "train_loss_mean: 6.03e-04\n",
      "train_loss_std: 1.00e-04\n",
      "val_loss_mean: 1.70e-03\n",
      "val_loss_std: 7.96e-04\n",
      "train_accuracy_mean: 1.00e+02\n",
      "train_accuracy_std: 0.00e+00\n",
      "val_accuracy_mean: 1.00e+02\n",
      "val_accuracy_std: 0.00e+00\n",
      "-------------------------------------------------\n",
      "Starting params: {'eta': 0.02, 'lam': 0.1, 'epochs': 500, 'n_batch': 31}\n",
      "Results:\n",
      "train_loss_mean: 3.46e-03\n",
      "train_loss_std: 4.66e-04\n",
      "val_loss_mean: 6.82e-03\n",
      "val_loss_std: 2.49e-03\n",
      "train_accuracy_mean: 1.00e+02\n",
      "train_accuracy_std: 0.00e+00\n",
      "val_accuracy_mean: 1.00e+02\n",
      "val_accuracy_std: 0.00e+00\n",
      "-------------------------------------------------\n",
      "Starting params: {'eta': 0.03, 'lam': 0.0, 'epochs': 500, 'n_batch': 31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\UNI\\LM_1\\MachineLearning\\ML_prj\\notebooks\\functions.py:91: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (target * np.log(layer) + (1 - target) * np.log(1 - layer))/target.shape[1]\n",
      "c:\\Users\\HP\\Desktop\\UNI\\LM_1\\MachineLearning\\ML_prj\\notebooks\\functions.py:91: RuntimeWarning: invalid value encountered in multiply\n",
      "  return - (target * np.log(layer) + (1 - target) * np.log(1 - layer))/target.shape[1]\n",
      "c:\\Users\\HP\\Desktop\\UNI\\LM_1\\MachineLearning\\ML_prj\\notebooks\\functions.py:94: RuntimeWarning: invalid value encountered in divide\n",
      "  return (layer - target)/(layer*(1-layer)*target.shape[1])\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2455: RuntimeWarning: invalid value encountered in relu (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:2455: RuntimeWarning: invalid value encountered in d_relu (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "train_loss_mean: nan\n",
      "train_loss_std: nan\n",
      "val_loss_mean: nan\n",
      "val_loss_std: nan\n",
      "train_accuracy_mean: 8.99e+01\n",
      "train_accuracy_std: 2.02e+01\n",
      "val_accuracy_mean: 9.04e+01\n",
      "val_accuracy_std: 1.92e+01\n",
      "-------------------------------------------------\n",
      "Starting params: {'eta': 0.03, 'lam': 0.1, 'epochs': 500, 'n_batch': 31}\n",
      "Results:\n",
      "train_loss_mean: 1.98e-03\n",
      "train_loss_std: 6.09e-04\n",
      "val_loss_mean: 4.22e-03\n",
      "val_loss_std: 2.16e-03\n",
      "train_accuracy_mean: 1.00e+02\n",
      "train_accuracy_std: 0.00e+00\n",
      "val_accuracy_mean: 9.83e+01\n",
      "val_accuracy_std: 3.33e+00\n",
      "-------------------------------------------------\n",
      "Best combination of parameters: {'eta': 0.01, 'lam': 0.0, 'epochs': 500, 'n_batch': 31, 'results': {'train_loss_mean': 0.0010398758323497928, 'train_loss_std': 0.000170173258345776, 'val_loss_mean': 0.0025654108589527403, 'val_loss_std': 0.0005040053716545955, 'train_accuracy_mean': 100.0, 'train_accuracy_std': 0.0, 'val_accuracy_mean': 100.0, 'val_accuracy_std': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "best_comb = grid_search(X_train, y_train.reshape((1,124)), params, 5, [accuracy], callbacks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4215045574.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    a = {'aa':1, 'cc':>}\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = {'aa':1, 'cc':}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.less_equal(3,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
